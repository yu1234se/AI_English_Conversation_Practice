import base64
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"  # ã“ã‚Œã‚’è¿½åŠ 

import re
import io
import tempfile
import numpy as np
import sounddevice as sd
import soundfile as sf
import streamlit as st
from audio_transcribe import AudioTranscriber
from kokoro import KPipeline
from conversation_agent import generate_response



def generate_audio(pipeline, text, speed=1.0):
    """éŸ³å£°ã‚’ç”Ÿæˆã—ã¦ãƒã‚¤ãƒˆåˆ—ã‚’è¿”ã™ï¼ˆé€Ÿåº¦èª¿æ•´ä»˜ãï¼‰"""
    generator = pipeline(text, voice='af_heart')
    chunks = [audio for _, _, audio in generator]
    full_audio = np.concatenate(chunks, axis=0)
    
    # é€Ÿåº¦èª¿æ•´
    if speed != 1.0:
        new_length = int(len(full_audio) / speed)
        indices = np.clip(np.round(np.arange(new_length) * speed), 0, len(full_audio)-1).astype(int)
        full_audio = full_audio[indices]
    
    buffer = io.BytesIO()
    sf.write(buffer, full_audio, 24000, format='WAV')
    return buffer.getvalue()

def custom_audio_recorder():
    """ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ãƒ¬ã‚³ãƒ¼ãƒ€ãƒ¼"""
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ¤ Start Recording", disabled=st.session_state.recording_active,
                    use_container_width=True, key="start_recording"):
            st.session_state.recording_active = True
            st.session_state.audio_data = None
            st.rerun()
    
    with col2:
        if st.button("â¹ï¸ Stop Recording", disabled=not st.session_state.recording_active,
                    use_container_width=True, key="stop_recording"):
            st.session_state.recording_active = False
            st.rerun()
    
    # éŒ²éŸ³å‡¦ç†
    if st.session_state.recording_active:
        st.info("Recording... Speak now! (Max 5 minutes)")
        fs = 16000
        seconds = 300
        
        # éŒ²éŸ³é–‹å§‹
        recording = sd.rec(int(seconds * fs), samplerate=fs, channels=1, dtype='float32')
        st.session_state.recording = recording
        st.session_state.fs = fs
        st.session_state.recording_in_progress = True
    
    # éŒ²éŸ³åœæ­¢å¾Œã®å‡¦ç†
    if not st.session_state.recording_active and 'recording' in st.session_state:
        recording = st.session_state.recording
        
        # ç„¡éŸ³éƒ¨åˆ†ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
        audio_data = recording.squeeze()
        threshold = 0.01
        non_silent = np.where(np.abs(audio_data) > threshold)[0]
        
        if len(non_silent) > 0:
            start = max(0, non_silent[0] - 100)
            end = min(len(audio_data), non_silent[-1] + 100)
            audio_data = audio_data[start:end]
        else:
            audio_data = np.array([])
        
        if len(audio_data) > 0:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmpfile:
                tmpfile_path = tmpfile.name
                sf.write(tmpfile_path, audio_data, st.session_state.fs, format='WAV')
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            with open(tmpfile_path, "rb") as f:
                st.session_state.audio_data = f.read()
            st.session_state.audio_path = tmpfile_path
            
            # éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.audio(st.session_state.audio_data, format="audio/wav")
        else:
            st.warning("No audio recorded. Please try again.")
        
        # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
        del st.session_state.recording
    
    return st.session_state.get('audio_data') is not None

def transcribe_audio(audio_path):
    """éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—"""
    transcriber = AudioTranscriber()
    segments = transcriber.transcribe(audio_path)
    return " ".join(text for _, _, text in segments)

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
st.title("AI English Conversation Practice")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []
if "pipeline" not in st.session_state:
    # US Englishã«å›ºå®š (lang_code='a')
    st.session_state.pipeline = KPipeline(lang_code='a')
if "recording_active" not in st.session_state:
    st.session_state.recording_active = False
if "audio_data" not in st.session_state:
    st.session_state.audio_data = None
if "audio_speed" not in st.session_state:
    st.session_state.audio_speed = 1.0

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šï¼ˆéŸ³å£°é€Ÿåº¦ã®ã¿ï¼‰
with st.sidebar:
    st.subheader("Settings")
    # éŸ³å£°é€Ÿåº¦èª¿æ•´
    st.session_state.audio_speed = st.slider(
        "Speech Speed",
        min_value=0.5,
        max_value=2.0,
        value=1.0,
        step=0.1,
        help="Adjust the playback speed of AI responses"
    )

# ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
if len(st.session_state.messages) == 0:
    st.info("Click 'Start Recording' to begin your English conversation practice!")

# éŒ²éŸ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.header("ğŸ¤ Record Your Voice")
if custom_audio_recorder() and st.button("Transcribe and Send"):
    with st.spinner("Transcribing your speech..."):
        transcribed_text = transcribe_audio(st.session_state.audio_path)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        try:
            os.unlink(st.session_state.audio_path)
        except:
            pass
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        st.session_state.messages.append({
            "role": "user", 
            "content": transcribed_text,
            "type": "voice"
        })
        
        # çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
        st.session_state.audio_data = None
        st.session_state.audio_path = None

# ä¼šè©±å‡¦ç†
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    last_user_msg = st.session_state.messages[-1]["content"]
    with st.spinner("AI is thinking..."):
        # AIå¿œç­”ç”Ÿæˆï¼ˆåˆ†é›¢ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼‰
        ai_response = generate_response(
            last_user_msg, 
            st.session_state.messages
        )
        
        # éŸ³å£°ç”Ÿæˆï¼ˆé€Ÿåº¦èª¿æ•´ä»˜ãï¼‰
        audio_bytes = generate_audio(
            st.session_state.pipeline, 
            ai_response,
            speed=st.session_state.audio_speed
        )
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        st.session_state.messages.append({
            "role": "assistant", 
            "content": ai_response,
            "audio_bytes": audio_bytes
        })
        
    # ç”»é¢æ›´æ–°
    st.rerun()

# ä¼šè©±è¡¨ç¤º
for i, msg in enumerate(st.session_state.messages):
    with st.chat_message(msg["role"]):
        if msg["role"] == "user":
            if msg["type"] == "voice":
                st.caption("ğŸ¤ You said:")
            st.write(msg["content"])
        else:
            # éŸ³å£°ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ (æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è‡ªå‹•å†ç”Ÿ)
            autoplay = (i == len(st.session_state.messages) - 1)
            st.audio(msg["audio_bytes"], format='audio/wav', autoplay=autoplay)
            
            # æŠ˜ã‚ŠãŸãŸã¿å¼ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            with st.expander("Click to see the English text"):
                st.write(msg["content"])
